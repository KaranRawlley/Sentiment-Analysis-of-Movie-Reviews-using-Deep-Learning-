{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from numpy import array \n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten,merge\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Input,RepeatVector,Permute,Multiply,Lambda,Bidirectional\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Well, you'd better if you plan on sitting thro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Moonwalker is a Fantasy Music film staring Mic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>I bought this video on a throw-out table at th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Since the last horrid Astérix film and the fac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>I grew up with the Abbott and Costello movies,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  sentiment\n",
       "0  Well, you'd better if you plan on sitting thro...          0\n",
       "1  Moonwalker is a Fantasy Music film staring Mic...          1\n",
       "2  I bought this video on a throw-out table at th...          0\n",
       "3  Since the last horrid Astérix film and the fac...          1\n",
       "4  I grew up with the Abbott and Costello movies,...          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('reviews.csv')\n",
    "df.isnull().values.any() # checks if data file have any null values\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "  text = re.sub('<[^>]*>', '', text)\n",
    "  emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "  text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
    "      ' '.join(emoticons).replace('-', '')\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "sentences = list(df['reviews'])\n",
    "for sen in sentences:\n",
    "    X.append(preprocess(sen))\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85, 21, 261, 30, 4, 1, 1573, 826, 10, 15, 4822, 424, 7, 6, 90, 424, 1, 586, 6, 1337, 2, 424, 1, 114, 6, 1420, 2221, 192, 10, 6, 3, 468, 4, 34, 412, 1254, 492, 2, 45, 11, 6, 125, 148, 91, 1800, 93, 20, 79, 365, 10, 28, 39, 25, 191, 49, 1427, 5, 10, 15, 5, 52, 73, 16, 1, 16, 28, 1, 563, 33, 3, 93, 1806, 1453, 186, 6, 53, 951, 2, 2417, 1, 564, 922, 1, 273, 6, 3, 997, 37, 1, 713, 4561, 4, 89, 98, 2, 236, 18, 23, 222, 1, 925, 6, 47, 90, 1, 19, 16, 68, 1, 1193, 4, 10, 19, 96, 27, 221, 3, 72, 127, 297, 4900, 1, 271, 5, 94, 7, 52, 818, 14, 3, 1095, 192, 45, 20, 50, 10, 2964, 1, 925, 6, 81, 632, 30, 616, 8, 3, 646, 2638, 1338, 41, 1035, 5, 29, 3047, 177, 7, 4, 3857, 51, 149, 1683, 125, 1574, 5, 77, 48, 4, 1, 159, 2, 20, 203, 365, 10, 19, 14, 72, 14, 9, 119]\n",
      "180\n",
      "244\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "print(X_train[0])\n",
    "print(len(X_train[0]))\n",
    "print(len(X_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding process \n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 100\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "glove_file = open('glove.6B.300d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = zeros((vocab_size, 300))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94562, 300)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "units=200\n",
    "vocab_size = embedding_matrix.shape[0]\n",
    "embedding_size = embedding_matrix.shape[1]\n",
    "\n",
    "\n",
    "_input = Input(shape=[maxlen], dtype='int32')\n",
    "\n",
    "# get the embedding layer\n",
    "embedded = Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_size,\n",
    "        input_length=maxlen,\n",
    "        trainable=False,\n",
    "        mask_zero=False,\n",
    "        weights=[embedding_matrix]\n",
    "    )(_input)\n",
    "\n",
    "activations = LSTM(units, return_sequences=True)(embedded)\n",
    "activations = Dropout(0.3)(activations)\n",
    "\n",
    "# compute importance for each step\n",
    "attention = Dense(1, activation='tanh')(activations) \n",
    "attention = Flatten()(attention)\n",
    "attention = Activation('softmax')(attention)\n",
    "attention = RepeatVector(units)(attention)\n",
    "attention = Permute([2, 1])(attention)\n",
    "\n",
    "# apply the attention\n",
    "sent_representation = Multiply()([activations, attention])\n",
    "sent_representation = Lambda(lambda xin: K.sum(xin, axis=-2), output_shape=(units,))(sent_representation)\n",
    "sent_representation = Dropout(0.4)(sent_representation)\n",
    "\n",
    "probabilities = Dense(1, activation='sigmoid')(sent_representation)\n",
    "\n",
    "model = Model(_input,probabilities)\n",
    "checkpoint = ModelCheckpoint(filepath=\"weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\", verbose=1, monitor='val_loss',save_best_only=True, mode='auto')\n",
    "callbacks_list = [checkpoint]\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/15\n",
      "40000/40000 [==============================] - 65s 2ms/step - loss: 0.4317 - accuracy: 0.7941 - val_loss: 0.3863 - val_accuracy: 0.8209\n",
      "Epoch 2/15\n",
      "40000/40000 [==============================] - 72s 2ms/step - loss: 0.3420 - accuracy: 0.8491 - val_loss: 0.3356 - val_accuracy: 0.8508\n",
      "Epoch 3/15\n",
      "40000/40000 [==============================] - 75s 2ms/step - loss: 0.3110 - accuracy: 0.8656 - val_loss: 0.3327 - val_accuracy: 0.8539\n",
      "Epoch 4/15\n",
      "40000/40000 [==============================] - 74s 2ms/step - loss: 0.2805 - accuracy: 0.8827 - val_loss: 0.3161 - val_accuracy: 0.8650\n",
      "Epoch 5/15\n",
      "40000/40000 [==============================] - 77s 2ms/step - loss: 0.2493 - accuracy: 0.8973 - val_loss: 0.3025 - val_accuracy: 0.8702\n",
      "Epoch 6/15\n",
      "40000/40000 [==============================] - 189s 5ms/step - loss: 0.2179 - accuracy: 0.9112 - val_loss: 0.3152 - val_accuracy: 0.8660\n",
      "Epoch 7/15\n",
      "40000/40000 [==============================] - 308s 8ms/step - loss: 0.1859 - accuracy: 0.9259 - val_loss: 0.3245 - val_accuracy: 0.8705\n",
      "Epoch 8/15\n",
      "40000/40000 [==============================] - 182s 5ms/step - loss: 0.1504 - accuracy: 0.9418 - val_loss: 0.3586 - val_accuracy: 0.8690\n",
      "Epoch 9/15\n",
      "40000/40000 [==============================] - 73s 2ms/step - loss: 0.1158 - accuracy: 0.9557 - val_loss: 0.4136 - val_accuracy: 0.8638\n",
      "Epoch 10/15\n",
      "40000/40000 [==============================] - 74s 2ms/step - loss: 0.0819 - accuracy: 0.9699 - val_loss: 0.4796 - val_accuracy: 0.8610\n",
      "Epoch 11/15\n",
      "40000/40000 [==============================] - 75s 2ms/step - loss: 0.0619 - accuracy: 0.9775 - val_loss: 0.5664 - val_accuracy: 0.8594\n",
      "Epoch 12/15\n",
      "40000/40000 [==============================] - 74s 2ms/step - loss: 0.0439 - accuracy: 0.9853 - val_loss: 0.6503 - val_accuracy: 0.8601\n",
      "Epoch 13/15\n",
      "40000/40000 [==============================] - 76s 2ms/step - loss: 0.0367 - accuracy: 0.9869 - val_loss: 0.6252 - val_accuracy: 0.8544\n",
      "Epoch 14/15\n",
      "40000/40000 [==============================] - 75s 2ms/step - loss: 0.0310 - accuracy: 0.9896 - val_loss: 0.6437 - val_accuracy: 0.8597\n",
      "Epoch 15/15\n",
      "40000/40000 [==============================] - 75s 2ms/step - loss: 0.0260 - accuracy: 0.9911 - val_loss: 0.6466 - val_accuracy: 0.8624\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x22f2da91908>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=15, batch_size=64, validation_data = (X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-tensorflow]",
   "language": "python",
   "name": "conda-env-.conda-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
